# üîÑ Pipeline ETL ‚Äì Integra√ß√£o de Dados de Cliente

Bem-vindo ao reposit√≥rio de ETL para integra√ß√£o e transforma√ß√£o de dados de clientes utilizando o [Pentaho Data Integration (Kettle)](https://community.hitachivantara.com/s/article/downloads). Este projeto inclui orquestra√ß√£o de jobs, carregamento de dados, agrega√ß√µes e mecanismos de logging para suportar fluxos de dados robustos.

## üìÅ Estrutura do Projeto

| Arquivo/Fonte                      | Descri√ß√£o                                           |
|-----------------------------------|-----------------------------------------------------|
| `Carga_Cliente.ktr`               | Transforma√ß√£o para carga de dados de clientes       |
| `Carga_Dados.ktr`                 | Transforma√ß√£o para carga geral de dados             |
| `IntegracaoJob.kjb`               | Job principal que orquestra as transforma√ß√µes       |
| `Transformation_Init_Jobs.ktr`    | Inicializa√ß√£o dos jobs e controle de execu√ß√£o       |
| `Transformation Agregacoes.ktr`   | L√≥gica de agrega√ß√µes para relat√≥rios ou an√°lises    |
| `Transformation_Logging.ktr`      | Transforma√ß√£o de logging para auditoria             |
| `logging.db`                      | Banco de dados SQLite para armazenar logs           |
| `shared.xml`                      | Configura√ß√µes compartilhadas (conex√µes, vari√°veis)  |
| `shared.xml.backup`              | Backup das configura√ß√µes compartilhadas             |
| `sql_script.sql`                  | Script SQL para cria√ß√£o ou migra√ß√£o de banco        |
| `README.md`                       | Documenta√ß√£o do projeto                             |

## ‚öôÔ∏è Vis√£o Funcional

- **Habilitar/Desabilitar Fluxo**: A maioria das transforma√ß√µes possui l√≥gica para ativar ou desativar fluxos com base em par√¢metros.
- **Agrega√ß√µes**: Processamento de dados para relat√≥rios e an√°lises.
- **Logging**: Logs de execu√ß√£o s√£o armazenados em `logging.db` para rastreabilidade.
- **Orquestra√ß√£o de Jobs**: `IntegracaoJob.kjb` coordena a execu√ß√£o das transforma√ß√µes na ordem correta.

## üöÄ Como Executar

### Pr√©-requisitos

- [Pentaho Data Integration (PDI)](https://sourceforge.net/projects/pentaho/)
- Java Runtime Environment (JRE)
- SQLite (opcional, para visualizar `logging.db`)

### Execu√ß√£o

1. Abra o Spoon e carregue o arquivo `IntegracaoJob.kjb`.
2. Verifique se o `shared.xml` est√° configurado corretamente com suas conex√µes de banco.
3. Execute o job e acompanhe o progresso pelo Spoon ou pelos logs.

### Logs

- Os logs s√£o gravados em `logging.db` via `Transformation_Logging.ktr`.
- Voc√™ pode consultar o banco usando qualquer visualizador SQLite.

## üß™ Notas de Desenvolvimento

- As transforma√ß√µes s√£o modulares e podem ser executadas individualmente.
- Use par√¢metros para controlar a execu√ß√£o de fluxos (ex.: ativar/desativar carga de cliente).
- Configura√ß√µes compartilhadas est√£o em `shared.xml`, com backup em `shared.xml.backup`.
